{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, plot_confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_msg_box(msg, indent=1, width=None, title=None):\n",
    "    \"\"\"Print message-box with optional title.\"\"\"\n",
    "    lines = msg.split('\\n')\n",
    "    space = \" \" * indent\n",
    "    if not width:\n",
    "        width = max(map(len, lines))\n",
    "    box = f'╔{\"═\" * (width + indent * 2)}╗\\n'  # upper_border\n",
    "    if title:\n",
    "        box += f'║{space}{title:<{width}}{space}║\\n'  # title\n",
    "        box += f'║{space}{\"-\" * len(title):<{width}}{space}║\\n'  # underscore\n",
    "    box += ''.join([f'║{space}{line:<{width}}{space}║\\n' for line in lines])\n",
    "    box += f'╚{\"═\" * (width + indent * 2)}╝'  # lower_border\n",
    "    print(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINDATA_PATH = 'data/train_tweets_removed.csv'\n",
    "TESTDATA_PATH = 'data/testdata.manual.2009.06.14_new.csv'\n",
    "BASIC_COLUMN_HEADERS = ['polarity', 'tweet_id',\n",
    "                        'date', 'query', 'user', 'content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions for cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('contractions.json')\n",
    "contractions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_repeated_letters(word: str):\n",
    "    currentC = ''\n",
    "    count = 1\n",
    "    new_word = \"\"\n",
    "    for i, c in enumerate(word):\n",
    "        if c == currentC:\n",
    "            count += 1\n",
    "            if count <= 2:\n",
    "                new_word += c\n",
    "        else:\n",
    "            new_word += c\n",
    "            count = 1\n",
    "        currentC = c\n",
    "    return new_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, remove_usernames: bool, remove_urls: bool):\n",
    "    #lowercase\n",
    "    text = text.lower()\n",
    "    #remove usernames\n",
    "    if remove_usernames:\n",
    "        text = re.sub(r'@[^\\s]+', 'USERNAME', text)\n",
    "    #remove urls https://www.geeksforgeeks.org/python-check-url-string/\n",
    "    if remove_urls:\n",
    "        text = re.sub(\n",
    "            r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\", 'URL', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_tokens(token_array, remove_repeated_letters: bool, translate_contractions: bool):\n",
    "    for i, word in enumerate(token_array):\n",
    "        processed_word = word\n",
    "        #translate contractions\n",
    "        if translate_contractions:\n",
    "            if word in contractions:\n",
    "                processed_word = contractions[word]\n",
    "        #repeated letters\n",
    "        if remove_repeated_letters:\n",
    "            processed_word = transform_repeated_letters(processed_word)\n",
    "\n",
    "        token_array[i] = processed_word\n",
    "    return token_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "def clean_text(text: str, remove_usernames: bool, remove_urls: bool, remove_repeated_letters: bool, translate_contractions: bool):\n",
    "    text = preprocess_text(text, remove_usernames, remove_urls)\n",
    "    #tokenize\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    tokenized_text = preprocessing_tokens(tokenized_text, remove_repeated_letters, translate_contractions)\n",
    "    #detokenize for Vectorizer because it can only work with full strings\n",
    "    detokenized_text = detokenizer.detokenize(tokenized_text)\n",
    "    return detokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    testdata = pd.read_csv(TESTDATA_PATH, delimiter=';', header=None)\n",
    "    testdata.columns = BASIC_COLUMN_HEADERS\n",
    "    #delete all with polarity 2 (neutral)\n",
    "    testdata = testdata[testdata['polarity'] != 2]\n",
    "    return testdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(remove_usernames: bool, remove_urls: bool, remove_repeated_letters: bool, translate_contractions: bool, remove_stop_words: bool, preprocessing_row: int):\n",
    "    \n",
    "    msg = \"Removing Usernames: %s\\n\" \\\n",
    "        \"Removing URL's: %s\\n\" \\\n",
    "        \"Removing repeated letters: %s \\n\" \\\n",
    "        \"Tranlate contractions: %s \\n\" \\\n",
    "        \"Removing stop words: %s\\n\" \\\n",
    "        \"Resulting preprocessing row: %d\\n\" % (remove_usernames, remove_urls, remove_repeated_letters,\n",
    "                                               translate_contractions, remove_stop_words, preprocessing_row)\n",
    "\n",
    "\n",
    "    print_msg_box(msg=msg, indent=2, title='Chosen parameters:')\n",
    "\n",
    "\n",
    "    #load train data\n",
    "    print(\"\\nLoad train data from %s\" % TRAINDATA_PATH)\n",
    "    train_data = pd.read_csv(TRAINDATA_PATH)\n",
    "    train_data['content'] = train_data['content'].values.astype('str')\n",
    "\n",
    "    #apply cleaning to train data\n",
    "    print(\"Preprocess train data...\")\n",
    "    train_data['content'] = train_data['content'].apply(lambda x: clean_text(\n",
    "        x, remove_usernames=remove_usernames, remove_urls=remove_urls, \n",
    "        remove_repeated_letters=remove_repeated_letters, translate_contractions=translate_contractions))\n",
    "\n",
    "    #save preprocessed data to csv\n",
    "    train_data.to_csv(\"data/csv_rows/train_row_%d.csv\" % (preprocessing_row))\n",
    "    print(\"Save preprocessed train data to: data/csv_rows/train_row_%d.csv\" %\n",
    "          preprocessing_row)\n",
    "\n",
    "    #print out data head\n",
    "    sw_message_str = \"(stopwords will be removed later)\" if remove_stop_words else \"\"\n",
    "    print('Train dataset after cleaning %s:' % sw_message_str)\n",
    "    print(train_data.loc[:10, 'content'])\n",
    "\n",
    "    #load test data\n",
    "    print(\"\\nLoad test data from %s\" % TESTDATA_PATH)\n",
    "    test_data = load_test_data()\n",
    "    #apply cleaning to testdata\n",
    "    print(\"Preprocess test data...\")\n",
    "    test_data['content'] = test_data['content'].apply(lambda x: clean_text(\n",
    "        x, remove_usernames=remove_usernames, remove_urls=remove_urls,\n",
    "        remove_repeated_letters=remove_repeated_letters, translate_contractions=translate_contractions))\n",
    "\n",
    "    #save preprocessed data to csv\n",
    "    print(\"Save preprocessed test data to: data/csv_rows/test_row_%d.csv\" %\n",
    "          (preprocessing_row))\n",
    "    test_data.to_csv(\"data/csv_rows/test_row_%d.csv\" % (preprocessing_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row 1: No preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔═════════════════════════════════════╗\n",
      "║  Chosen parameters:                 ║\n",
      "║  ------------------                 ║\n",
      "║  Removing Usernames: False          ║\n",
      "║  Removing URL's: False              ║\n",
      "║  Removing repeated letters: False   ║\n",
      "║  Tranlate contractions: False       ║\n",
      "║  Removing stop words: False         ║\n",
      "║  Resulting preprocessing row: 1     ║\n",
      "║                                     ║\n",
      "╚═════════════════════════════════════╝\n",
      "\n",
      "Load train data from data/train_tweets_removed.csv\n",
      "Preprocess train data...\n",
      "Save preprocessed train data to: data/csv_rows/train_row_1.csv\n",
      "Train dataset after cleaning :\n",
      "0     switchfoot httptwitpiccom2y1zl awww thats a bu...\n",
      "1     is upset that he cant update his facebook by t...\n",
      "2     kenichan i dived many times for the ball manag...\n",
      "3        my whole body feels itchy and like its on fire\n",
      "4     nationwideclass no its not behaving at all im ...\n",
      "5                           kwesidei not the whole crew\n",
      "6                                            need a hug\n",
      "7     loltrish hey long time no see yes rains a bit ...\n",
      "8                     tatiana_k nope they didnt have it\n",
      "9                                 twittera que me muera\n",
      "10               spring break in plain city its snowing\n",
      "Name: content, dtype: object\n",
      "\n",
      "Load test data from data/testdata.manual.2009.06.14_new.csv\n",
      "Preprocess test data...\n",
      "Save preprocessed test data to: data/csv_rows/test_row_1.csv\n"
     ]
    }
   ],
   "source": [
    "preprocess(remove_usernames=False, remove_urls=False, remove_repeated_letters=False,\n",
    "                   translate_contractions=False, remove_stop_words=False, preprocessing_row=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row 2: u, l, rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔════════════════════════════════════╗\n",
      "║  Chosen parameters:                ║\n",
      "║  ------------------                ║\n",
      "║  Removing Usernames: True          ║\n",
      "║  Removing URL's: True              ║\n",
      "║  Removing repeated letters: True   ║\n",
      "║  Tranlate contractions: False      ║\n",
      "║  Removing stop words: False        ║\n",
      "║  Resulting preprocessing row: 2    ║\n",
      "║                                    ║\n",
      "╚════════════════════════════════════╝\n",
      "\n",
      "Load train data from data/train_tweets_removed.csv\n",
      "Preprocess train data...\n",
      "Save preprocessed train data to: data/csv_rows/train_row_2.csv\n",
      "Train dataset after cleaning :\n",
      "0     USERNAME URL aww thats a bummer you shoulda go...\n",
      "1     is upset that he cant update his facebook by t...\n",
      "2     USERNAME i dived many times for the ball manag...\n",
      "3        my whole body feels itchy and like its on fire\n",
      "4     USERNAME no its not behaving at all im mad why...\n",
      "5                           USERNAME not the whole crew\n",
      "6                                            need a hug\n",
      "7     USERNAME hey long time no see yes rains a bit ...\n",
      "8                      USERNAME nope they didnt have it\n",
      "9                                 USERNAME que me muera\n",
      "10               spring break in plain city its snowing\n",
      "Name: content, dtype: object\n",
      "\n",
      "Load test data from data/testdata.manual.2009.06.14_new.csv\n",
      "Preprocess test data...\n",
      "Save preprocessed test data to: data/csv_rows/test_row_2.csv\n"
     ]
    }
   ],
   "source": [
    "preprocess(remove_usernames=True, remove_urls=True, remove_repeated_letters=True,\n",
    "                   translate_contractions=False, remove_stop_words=False, preprocessing_row=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row 3: u, l, rl, sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔════════════════════════════════════╗\n",
      "║  Chosen parameters:                ║\n",
      "║  ------------------                ║\n",
      "║  Removing Usernames: True          ║\n",
      "║  Removing URL's: True              ║\n",
      "║  Removing repeated letters: True   ║\n",
      "║  Tranlate contractions: False      ║\n",
      "║  Removing stop words: True         ║\n",
      "║  Resulting preprocessing row: 3    ║\n",
      "║                                    ║\n",
      "╚════════════════════════════════════╝\n",
      "\n",
      "Load train data from data/train_tweets_removed.csv\n",
      "Preprocess train data...\n",
      "Save preprocessed train data to: data/csv_rows/train_row_3.csv\n",
      "Train dataset after cleaning (stopwords will be removed later):\n",
      "0     USERNAME URL aww thats a bummer you shoulda go...\n",
      "1     is upset that he cant update his facebook by t...\n",
      "2     USERNAME i dived many times for the ball manag...\n",
      "3        my whole body feels itchy and like its on fire\n",
      "4     USERNAME no its not behaving at all im mad why...\n",
      "5                           USERNAME not the whole crew\n",
      "6                                            need a hug\n",
      "7     USERNAME hey long time no see yes rains a bit ...\n",
      "8                      USERNAME nope they didnt have it\n",
      "9                                 USERNAME que me muera\n",
      "10               spring break in plain city its snowing\n",
      "Name: content, dtype: object\n",
      "\n",
      "Load test data from data/testdata.manual.2009.06.14_new.csv\n",
      "Preprocess test data...\n",
      "Save preprocessed test data to: data/csv_rows/test_row_3.csv\n"
     ]
    }
   ],
   "source": [
    "preprocess(remove_usernames=True, remove_urls=True, remove_repeated_letters=True,\n",
    "                   translate_contractions=False, remove_stop_words=True, preprocessing_row=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row 4: u, l, rl, abk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔════════════════════════════════════╗\n",
      "║  Chosen parameters:                ║\n",
      "║  ------------------                ║\n",
      "║  Removing Usernames: True          ║\n",
      "║  Removing URL's: True              ║\n",
      "║  Removing repeated letters: True   ║\n",
      "║  Tranlate contractions: True       ║\n",
      "║  Removing stop words: False        ║\n",
      "║  Resulting preprocessing row: 4    ║\n",
      "║                                    ║\n",
      "╚════════════════════════════════════╝\n",
      "\n",
      "Load train data from data/train_tweets_removed.csv\n",
      "Preprocess train data...\n",
      "Save preprocessed train data to: data/csv_rows/train_row_4.csv\n",
      "Train dataset after cleaning :\n",
      "0     USERNAME URL aww that is a bummer you shoulda ...\n",
      "1     is upset that he cannot update his facebook by...\n",
      "2     USERNAME i dived many times for the ball manag...\n",
      "3      my whole body feels itchy and like it is on fire\n",
      "4     USERNAME no it is not behaving at all i am mad...\n",
      "5                           USERNAME not the whole crew\n",
      "6                                            need a hug\n",
      "7     USERNAME hey long time no see yes rains a bit ...\n",
      "8                    USERNAME nope they did not have it\n",
      "9                                 USERNAME que me muera\n",
      "10             spring break in plain city it is snowing\n",
      "Name: content, dtype: object\n",
      "\n",
      "Load test data from data/testdata.manual.2009.06.14_new.csv\n",
      "Preprocess test data...\n",
      "Save preprocessed test data to: data/csv_rows/test_row_4.csv\n"
     ]
    }
   ],
   "source": [
    "preprocess(remove_usernames=True, remove_urls=True, remove_repeated_letters=True,\n",
    "                   translate_contractions=True, remove_stop_words=False, preprocessing_row=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row 5: u, l, rl, sw, abk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔════════════════════════════════════╗\n",
      "║  Chosen parameters:                ║\n",
      "║  ------------------                ║\n",
      "║  Removing Usernames: True          ║\n",
      "║  Removing URL's: True              ║\n",
      "║  Removing repeated letters: True   ║\n",
      "║  Tranlate contractions: True       ║\n",
      "║  Removing stop words: True         ║\n",
      "║  Resulting preprocessing row: 5    ║\n",
      "║                                    ║\n",
      "╚════════════════════════════════════╝\n",
      "\n",
      "Load train data from data/train_tweets_removed.csv\n",
      "Preprocess train data...\n",
      "Save preprocessed train data to: data/csv_rows/train_row_5.csv\n",
      "Train dataset after cleaning (stopwords will be removed later):\n",
      "0     USERNAME URL aww that is a bummer you shoulda ...\n",
      "1     is upset that he cannot update his facebook by...\n",
      "2     USERNAME i dived many times for the ball manag...\n",
      "3      my whole body feels itchy and like it is on fire\n",
      "4     USERNAME no it is not behaving at all i am mad...\n",
      "5                           USERNAME not the whole crew\n",
      "6                                            need a hug\n",
      "7     USERNAME hey long time no see yes rains a bit ...\n",
      "8                    USERNAME nope they did not have it\n",
      "9                                 USERNAME que me muera\n",
      "10             spring break in plain city it is snowing\n",
      "Name: content, dtype: object\n",
      "\n",
      "Load test data from data/testdata.manual.2009.06.14_new.csv\n",
      "Preprocess test data...\n",
      "Save preprocessed test data to: data/csv_rows/test_row_5.csv\n"
     ]
    }
   ],
   "source": [
    "preprocess(remove_usernames=True, remove_urls=True, remove_repeated_letters=True,\n",
    "                   translate_contractions=True, remove_stop_words=True, preprocessing_row = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "542bad02869b321a73a7da5260ee6f5456fa9b86f73b69a004054f84e7b0338b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
